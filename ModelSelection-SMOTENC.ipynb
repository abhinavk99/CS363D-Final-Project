{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, lets construct the different models that we will use.\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import svm\n",
    "from sklearn import neural_network\n",
    "from sklearn import ensemble\n",
    "\n",
    "# Read data-engineered CSV.\n",
    "df = pd.read_csv('H-2B_Engineered_Data.csv')\n",
    "labels = df.loc[:,'CASE_STATUS']\n",
    "features = pd.DataFrame(df.drop(labels = ['CASE_STATUS'], axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a pipeline for hosting three steps to take place on the training data within the cross-validation loop:\n",
    "\n",
    "Preprocessing (scaling numeric features, one-hot encoding categorical features), PCA (to reduce dimensionality, especially considering the expansion in dimensionality associated with the one-hot encoding), and class balancing.\n",
    "\n",
    "This notebook is used to test the SMOTENC class balancing method, which allows for synthetic minority oversampling on a dataset that contains both continuous and categorical features, as a means of performing class balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn import neighbors as knn\n",
    "from sklearn import pipeline\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "# Build a preprocessor to scale numeric features and one hot encode categorical features.\n",
    "numeric_features = ['NBR_WORKERS_REQUESTED', \n",
    "                    'BASIC_NUMBER_OF_HOURS', \n",
    "                    'BASIC_RATE_OF_PAY', \n",
    "                    'SUPERVISE_HOW_MANY', \n",
    "                    'NUM_OF_MONTHS_TRAINING',\n",
    "                    'EMP_EXP_NUM_MONTHS',\n",
    "                    'WORK_DAY_LENGTH']\n",
    "categorical_features = [\"AGENT_POC_EMP_REP_BY_AGENT\", \n",
    "                        \"SOC_CODE\", \n",
    "                        \"NAICS_CODE\", \n",
    "                        \"FULL_TIME_POSITION\", \n",
    "                        \"NATURE_OF_TEMPORARY_NEED\", \n",
    "                        \"EDUCATION_LEVEL\", \n",
    "                        \"SWA_NAME\", \n",
    "                        \"CITY_MATCH\", \n",
    "                        \"STATE_MATCH\", \n",
    "                        \"DAYTIME_WORK\", \n",
    "                        \"HAS_OVERTIME\"]\n",
    "\n",
    "# Get indices of numeric and categorical feature columns in dataframe.\n",
    "numeric_feature_names = set(numeric_features)\n",
    "categorical_feature_names = set(categorical_features)\n",
    "\n",
    "numeric_feature_indices = []\n",
    "categorical_feature_indices = []\n",
    "\n",
    "for index, column_name in enumerate(features.columns):\n",
    "    if column_name in numeric_feature_names:\n",
    "        numeric_feature_indices.append(index)\n",
    "    elif column_name in categorical_feature_names:\n",
    "        categorical_feature_indices.append(index)\n",
    "\n",
    "# Scale numeric features.\n",
    "numeric_transformer = pipeline.Pipeline(steps=[\n",
    "    ('scaler', pp.StandardScaler())])\n",
    "\n",
    "# One-hot encode categorical features.\n",
    "categorical_transformer = pipeline.Pipeline(steps=[\n",
    "    ('onehot', pp.OneHotEncoder(sparse = False, handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_feature_indices),\n",
    "        ('cat', categorical_transformer, categorical_feature_indices)])\n",
    "        \n",
    "random_state_val = 123\n",
    "smote = SMOTENC(sampling_strategy = 1, random_state = random_state_val, categorical_features = categorical_feature_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll be determining the best parameters for Decision Tree, K Nearest Neighbors, SVM, Random Forest Classifier, and MLP Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tree__criterion': 'entropy', 'tree__max_depth': 10, 'tree__max_features': 10, 'tree__min_samples_leaf': 10}\n",
      "0.8145259938837921\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "tuned_parameters = {\n",
    "    'tree__max_depth': [5, 10, 15, 20],\n",
    "    'tree__min_samples_leaf': [5, 10, 15, 20], \n",
    "    'tree__max_features': [5, 10, 15],\n",
    "    'tree__criterion': ['gini', 'entropy']\n",
    "}\n",
    "pipe = Pipeline(steps = [('smote', smote), ('preprocess', preprocessor), ('dim', decomposition.PCA()), ('tree', tree.DecisionTreeClassifier())])\n",
    "tree_model = GridSearchCV(pipe, param_grid=tuned_parameters, scoring='accuracy', cv=5)\n",
    "tree_model.fit(features, labels)\n",
    "print(tree_model.best_params_)\n",
    "print(tree_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'knn__n_neighbors': 1}\n",
      "0.8244648318042813\n"
     ]
    }
   ],
   "source": [
    "# K Nearest Neighbors\n",
    "tuned_parameters = {\n",
    "    'knn__n_neighbors': list(range(1, 25))\n",
    "}\n",
    "pipe = Pipeline(steps = [('smote', smote), ('preprocess', preprocessor), ('dim', decomposition.PCA()), ('knn', neighbors.KNeighborsClassifier())])\n",
    "knn_model = GridSearchCV(pipe, param_grid=tuned_parameters, scoring='accuracy', cv=5)\n",
    "knn_model.fit(features, labels)\n",
    "print(knn_model.best_params_)\n",
    "print(knn_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'svm__C': 1.5, 'svm__loss': 'hinge'}\n",
      "0.840519877675841\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Linear Support Vector Classifier\n",
    "tuned_parameters = {\n",
    "    \"svm__loss\": ['hinge', 'squared_hinge'],\n",
    "    'svm__C': [0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2],\n",
    "}\n",
    "\n",
    "svc = LinearSVC()\n",
    "\n",
    "pipe = Pipeline(steps = [('smote', smote), ('preprocess', preprocessor), ('dim', decomposition.PCA()), ('svm', svc)])\n",
    "svm_model = GridSearchCV(pipe, param_grid=tuned_parameters, scoring='accuracy', cv=5)\n",
    "svm_model.fit(features, labels)\n",
    "print(svm_model.best_params_)\n",
    "print(svm_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'net__activation': 'logistic', 'net__solver': 'sgd'}\n",
      "0.8414373088685015\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "tuned_parameters = {\n",
    "    'net__activation': ['logistic', 'tanh', 'relu'],\n",
    "    'net__solver': ['sgd', 'adam']\n",
    "}\n",
    "pipe = Pipeline(steps = [('smote', smote), ('preprocess', preprocessor), ('dim', decomposition.PCA()), ('net', neural_network.MLPClassifier())])\n",
    "net_model = GridSearchCV(pipe, param_grid=tuned_parameters, scoring='accuracy', cv=5)\n",
    "net_model.fit(features, labels)\n",
    "print(net_model.best_params_)\n",
    "print(net_model.best_score_)\n",
    "\n",
    "warnings.simplefilter(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rfc__max_depth': 44, 'rfc__max_features': 'sqrt', 'rfc__min_samples_leaf': 8}\n",
      "0.8451070336391437\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "# Random Forest Classifier\n",
    "tuned_parameters = {\n",
    "    'rfc__max_depth': list(range(35, 56)),\n",
    "    'rfc__min_samples_leaf': list(range(8, 13, 2)),\n",
    "    'rfc__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "pipe = Pipeline(steps = [('smote', smote), ('preprocess', preprocessor), ('dim', decomposition.PCA()), ('rfc', ensemble.RandomForestClassifier(n_estimators=10))])\n",
    "rfc_model = GridSearchCV(pipe, param_grid=tuned_parameters, scoring='accuracy', cv=5)\n",
    "rfc_model.fit(features, labels)\n",
    "print(rfc_model.best_params_)\n",
    "print(rfc_model.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create our classifiers based on the best tuned parameters.\n",
    "\n",
    "We use these classifiers to create an ensemble voting classifier, hosting a Decision Tree, Random Forest, KNN, Gaussian Naive Bayes, Linear Support Vector Classifier, and Neural Network classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "\n",
    "# Classifiers.\n",
    "tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=10, max_features=10, min_samples_leaf=10)\n",
    "classifiers.append(('tree', tree))\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=10, max_depth=44, max_features='sqrt', min_samples_leaf=8)\n",
    "classifiers.append(('rfc', rfc))\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "classifiers.append(('knn', knn))\n",
    "\n",
    "nb = naive_bayes.GaussianNB()\n",
    "classifiers.append(('nb', nb))\n",
    "\n",
    "sv = LinearSVC(C=1.5, loss='hinge')\n",
    "classifiers.append(('svm', sv))\n",
    "\n",
    "net = neural_network.MLPClassifier(activation='logistic', solver='sgd')\n",
    "classifiers.append(('neural_network', net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=10,\n",
      "                       max_features=10, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=10, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "[[ 434  355]\n",
      " [ 951 4800]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.55      0.40       789\n",
      "           1       0.93      0.83      0.88      5751\n",
      "\n",
      "    accuracy                           0.80      6540\n",
      "   macro avg       0.62      0.69      0.64      6540\n",
      "weighted avg       0.86      0.80      0.82      6540\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=44, max_features='sqrt', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=8, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "[[ 463  326]\n",
      " [ 756 4995]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.59      0.46       789\n",
      "           1       0.94      0.87      0.90      5751\n",
      "\n",
      "    accuracy                           0.83      6540\n",
      "   macro avg       0.66      0.73      0.68      6540\n",
      "weighted avg       0.87      0.83      0.85      6540\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
      "                     weights='uniform')\n",
      "[[ 373  416]\n",
      " [ 732 5019]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.47      0.39       789\n",
      "           1       0.92      0.87      0.90      5751\n",
      "\n",
      "    accuracy                           0.82      6540\n",
      "   macro avg       0.63      0.67      0.65      6540\n",
      "weighted avg       0.85      0.82      0.84      6540\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "[[ 275  514]\n",
      " [ 538 5213]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.35      0.34       789\n",
      "           1       0.91      0.91      0.91      5751\n",
      "\n",
      "    accuracy                           0.84      6540\n",
      "   macro avg       0.62      0.63      0.63      6540\n",
      "weighted avg       0.84      0.84      0.84      6540\n",
      "\n",
      "LinearSVC(C=1.5, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
      "          penalty='l2', random_state=None, tol=0.0001, verbose=0)\n",
      "[[ 512  277]\n",
      " [ 769 4982]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.65      0.49       789\n",
      "           1       0.95      0.87      0.90      5751\n",
      "\n",
      "    accuracy                           0.84      6540\n",
      "   macro avg       0.67      0.76      0.70      6540\n",
      "weighted avg       0.88      0.84      0.86      6540\n",
      "\n",
      "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
      "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "[[ 544  245]\n",
      " [ 797 4954]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.69      0.51       789\n",
      "           1       0.95      0.86      0.90      5751\n",
      "\n",
      "    accuracy                           0.84      6540\n",
      "   macro avg       0.68      0.78      0.71      6540\n",
      "weighted avg       0.89      0.84      0.86      6540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results from the Individual Classifiers.\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "for name, cl in classifiers:\n",
    "    pipe = Pipeline(steps = [('smote', smote), ('preprocess', preprocessor), ('dim', decomposition.PCA()), ('classifier', cl)])\n",
    "    pr = model_selection.cross_val_predict(pipe, features, labels, cv=5)\n",
    "    \n",
    "    print(cl)\n",
    "    print(metrics.confusion_matrix(labels, pr))\n",
    "    print(metrics.classification_report(labels, pr))\n",
    "    \n",
    "warnings.simplefilter(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier(estimators=[('tree',\n",
      "                              DecisionTreeClassifier(class_weight=None,\n",
      "                                                     criterion='entropy',\n",
      "                                                     max_depth=10,\n",
      "                                                     max_features=10,\n",
      "                                                     max_leaf_nodes=None,\n",
      "                                                     min_impurity_decrease=0.0,\n",
      "                                                     min_impurity_split=None,\n",
      "                                                     min_samples_leaf=10,\n",
      "                                                     min_samples_split=2,\n",
      "                                                     min_weight_fraction_leaf=0.0,\n",
      "                                                     presort=False,\n",
      "                                                     random_state=None,\n",
      "                                                     splitter='best')),\n",
      "                             ('rfc',\n",
      "                              RandomForestClassifier(boot...\n",
      "                                            beta_2=0.999, early_stopping=False,\n",
      "                                            epsilon=1e-08,\n",
      "                                            hidden_layer_sizes=(100,),\n",
      "                                            learning_rate='constant',\n",
      "                                            learning_rate_init=0.001,\n",
      "                                            max_iter=200, momentum=0.9,\n",
      "                                            n_iter_no_change=10,\n",
      "                                            nesterovs_momentum=True,\n",
      "                                            power_t=0.5, random_state=None,\n",
      "                                            shuffle=True, solver='sgd',\n",
      "                                            tol=0.0001, validation_fraction=0.1,\n",
      "                                            verbose=False, warm_start=False))],\n",
      "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
      "                 weights=None)\n",
      "[[ 513  276]\n",
      " [ 687 5064]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.65      0.52       789\n",
      "           1       0.95      0.88      0.91      5751\n",
      "\n",
      "    accuracy                           0.85      6540\n",
      "   macro avg       0.69      0.77      0.71      6540\n",
      "weighted avg       0.89      0.85      0.87      6540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creation of soft voting ensemble clssifier.\n",
    "voting = ensemble.VotingClassifier(classifiers)\n",
    "\n",
    "pipe = Pipeline(steps = [('smote', smote), ('preprocess', preprocessor), ('dim', decomposition.PCA()), ('classifier', voting)])\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "pr = model_selection.cross_val_predict(pipe, features, labels, cv=5)\n",
    "warnings.simplefilter(\"default\")\n",
    "    \n",
    "print(voting)\n",
    "print(metrics.confusion_matrix(labels, pr))\n",
    "print(metrics.classification_report(labels, pr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
