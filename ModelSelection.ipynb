{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, lets construct the different models that we will use.\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import svm\n",
    "from sklearn import neural_network\n",
    "from sklearn import ensemble\n",
    "\n",
    "# Read data-engineered CSV.\n",
    "df = pd.read_csv('H-2B_Engineered_Data.csv')\n",
    "labels = df.loc[:,'CASE_STATUS']\n",
    "features = pd.DataFrame(df.drop(labels = ['CASE_STATUS'], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn import neighbors as knn\n",
    "from sklearn import pipeline\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "# Build a preprocessor to scale numeric features and one hot encode categorical features.\n",
    "numeric_features = ['NBR_WORKERS_REQUESTED', \n",
    "                    'BASIC_NUMBER_OF_HOURS', \n",
    "                    'BASIC_RATE_OF_PAY', \n",
    "                    'SUPERVISE_HOW_MANY', \n",
    "                    'NUM_OF_MONTHS_TRAINING',\n",
    "                    'EMP_EXP_NUM_MONTHS',\n",
    "                    'WORK_DAY_LENGTH']\n",
    "categorical_features = [\"AGENT_POC_EMP_REP_BY_AGENT\", \n",
    "                        \"SOC_CODE\", \n",
    "                        \"NAICS_CODE\", \n",
    "                        \"FULL_TIME_POSITION\", \n",
    "                        \"NATURE_OF_TEMPORARY_NEED\", \n",
    "                        \"EDUCATION_LEVEL\", \n",
    "                        \"SWA_NAME\", \n",
    "                        \"CITY_MATCH\", \n",
    "                        \"STATE_MATCH\", \n",
    "                        \"DAYTIME_WORK\", \n",
    "                        \"HAS_OVERTIME\"]\n",
    "\n",
    "# Get indices of numeric and categorical feature columns in dataframe.\n",
    "numeric_feature_names = set(numeric_features)\n",
    "categorical_feature_names = set(categorical_features)\n",
    "\n",
    "numeric_feature_indices = []\n",
    "categorical_feature_indices = []\n",
    "\n",
    "for index, column_name in enumerate(features.columns):\n",
    "    if column_name in numeric_feature_names:\n",
    "        numeric_feature_indices.append(index)\n",
    "    elif column_name in categorical_feature_names:\n",
    "        categorical_feature_indices.append(index)\n",
    "\n",
    "# Scale numeric features.\n",
    "numeric_transformer = pipeline.Pipeline(steps=[\n",
    "    ('scaler', pp.StandardScaler())])\n",
    "\n",
    "# One-hot encode categorical features.\n",
    "categorical_transformer = pipeline.Pipeline(steps=[\n",
    "    ('onehot', pp.OneHotEncoder(sparse = False, handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_feature_indices),\n",
    "        ('cat', categorical_transformer, categorical_feature_indices)])\n",
    "        \n",
    "random_state_val = 123\n",
    "smote = SMOTENC(sampling_strategy = 1, random_state = random_state_val, categorical_features = categorical_feature_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll be determining the best parameters for Decision Tree, K Nearest Neighbors, SVM, Random Forest Classifier, and MLP Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tree__max_depth': 10}\n",
      "0.8140672782874617\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "tuned_parameters = {\n",
    "    'tree__max_depth': [5, 10, 15, 20],\n",
    "    'tree__min_samples_leaf': [5, 10, 15, 20], \n",
    "    'tree__max_features': [5, 10, 15],\n",
    "    'tree__criterion': ['gini', 'entropy']\n",
    "}\n",
    "pipe = Pipeline(steps = [('smote', smote), ('preprocess', preprocessor), ('dim', decomposition.PCA()), ('tree', tree.DecisionTreeClassifier())])\n",
    "tree_model = GridSearchCV(pipe, param_grid=tuned_parameters, scoring='accuracy', cv=5)\n",
    "tree_model.fit(features, labels)\n",
    "print(tree_model.best_params_)\n",
    "print(tree_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'knn__n_neighbors': 1}\n",
      "0.8844548774126239\n"
     ]
    }
   ],
   "source": [
    "# K Nearest Neighbors\n",
    "tuned_parameters = {\n",
    "    'knn__n_neighbors': list(range(1, 25))\n",
    "}\n",
    "pipe = Pipeline(steps = [('smote', smote), ('preprocess', preprocessor), ('dim', decomposition.PCA()), ('knn', neighbors.KNeighborsClassifier())])\n",
    "knn_model = GridSearchCV(pipe, param_grid=tuned_parameters, scoring='accuracy', cv=5)\n",
    "knn_model.fit(features, labels)\n",
    "print(knn_model.best_params_)\n",
    "print(knn_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'svm__C': 2, 'svm__kernel': 'rbf'}\n",
      "0.8172491740566858\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine\n",
    "tuned_parameters = {\n",
    "    'svm__C': [0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2],\n",
    "    'svm__kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "pipe = Pipeline(steps = [('smote', smote), ('preprocess', preprocessor), ('dim', decomposition.PCA()), ('svm', svm.SVC(gamma='auto'))])\n",
    "svm_model = GridSearchCV(pipe, param_grid=tuned_parameters, scoring='accuracy', cv=5)\n",
    "svm_model.fit(features, labels)\n",
    "print(svm_model.best_params_)\n",
    "print(svm_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'net__activation': 'relu', 'net__solver': 'adam'}\n",
      "0.8573291601460615\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "tuned_parameters = {\n",
    "    'net__activation': ['logistic', 'tanh', 'relu'],\n",
    "    'net__solver': ['sgd', 'adam']\n",
    "}\n",
    "pipe = Pipeline(steps = [('smote', smote), ('preprocess', preprocessor), ('dim', decomposition.PCA()), ('net', neural_network.MLPClassifier())])\n",
    "net_model = GridSearchCV(pipe, param_grid=tuned_parameters, scoring='accuracy', cv=5)\n",
    "net_model.fit(features, labels)\n",
    "print(net_model.best_params_)\n",
    "print(net_model.best_score_)\n",
    "\n",
    "warnings.simplefilter(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rfc__max_depth': 37, 'rfc__max_features': 'sqrt', 'rfc__min_samples_leaf': 8}\n",
      "0.8755868544600939\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "tuned_parameters = {\n",
    "    'rfc__max_depth': list(range(35, 56)),\n",
    "    'rfc__min_samples_leaf': list(range(8, 13, 2)),\n",
    "    'rfc__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "pipe = Pipeline(steps = [('smote', smote), ('preprocess', preprocessor), ('dim', decomposition.PCA()), ('rfc', ensemble.RandomForestClassifier(n_estimators=10))])\n",
    "rfc_model = GridSearchCV(pipe, param_grid=tuned_parameters, scoring='accuracy', cv=5)\n",
    "rfc_model.fit(features, labels)\n",
    "print(rfc_model.best_params_)\n",
    "print(rfc_model.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create our classifiers based on the best tuned parameters.\n",
    "\n",
    "We use these classifiers to create an ensemble voting classifier, hosting a Decision Tree, Random Forest, KNN, Gaussian Naive Bayes, Support Vvector Machine, and Neural Network classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "\n",
    "# Classifiers.\n",
    "tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=20, max_features=15, min_samples_leaf=5)\n",
    "classifiers.append(('tree', tree))\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=10, max_depth=37, max_features='sqrt', min_samples_leaf=8)\n",
    "classifiers.append(('rfc', rfc))\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "classifiers.append(('knn', knn))\n",
    "\n",
    "nb = naive_bayes.GaussianNB()\n",
    "classifiers.append(('nb', nb))\n",
    "\n",
    "sv = svm.SVC(gamma='auto', C=2.0, kernel='rbf')\n",
    "classifiers.append(('svm', sv))\n",
    "\n",
    "net = neural_network.MLPClassifier(activation='relu', solver='adam')\n",
    "classifiers.append(('neural_network', net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=20,\n",
      "                       max_features=15, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=5, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "[[4933  818]\n",
      " [ 994 4757]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84      5751\n",
      "           1       0.85      0.83      0.84      5751\n",
      "\n",
      "    accuracy                           0.84     11502\n",
      "   macro avg       0.84      0.84      0.84     11502\n",
      "weighted avg       0.84      0.84      0.84     11502\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=37, max_features='sqrt', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=8, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "[[5044  707]\n",
      " [ 795 4956]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87      5751\n",
      "           1       0.88      0.86      0.87      5751\n",
      "\n",
      "    accuracy                           0.87     11502\n",
      "   macro avg       0.87      0.87      0.87     11502\n",
      "weighted avg       0.87      0.87      0.87     11502\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
      "                     weights='uniform')\n",
      "[[5144  607]\n",
      " [ 722 5029]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89      5751\n",
      "           1       0.89      0.87      0.88      5751\n",
      "\n",
      "    accuracy                           0.88     11502\n",
      "   macro avg       0.88      0.88      0.88     11502\n",
      "weighted avg       0.88      0.88      0.88     11502\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "[[1909 3842]\n",
      " [1087 4664]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.33      0.44      5751\n",
      "           1       0.55      0.81      0.65      5751\n",
      "\n",
      "    accuracy                           0.57     11502\n",
      "   macro avg       0.59      0.57      0.55     11502\n",
      "weighted avg       0.59      0.57      0.55     11502\n",
      "\n",
      "SVC(C=2.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[[4424 1327]\n",
      " [ 775 4976]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.77      0.81      5751\n",
      "           1       0.79      0.87      0.83      5751\n",
      "\n",
      "    accuracy                           0.82     11502\n",
      "   macro avg       0.82      0.82      0.82     11502\n",
      "weighted avg       0.82      0.82      0.82     11502\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "[[4886  865]\n",
      " [ 780 4971]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.86      5751\n",
      "           1       0.85      0.86      0.86      5751\n",
      "\n",
      "    accuracy                           0.86     11502\n",
      "   macro avg       0.86      0.86      0.86     11502\n",
      "weighted avg       0.86      0.86      0.86     11502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results from the Individual Classifiers.\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "for name, cl in classifiers:\n",
    "    pipe = Pipeline(steps = [('smote', smote), ('preprocess', preprocessor), ('dim', decomposition.PCA()), ('classifier', cl)])\n",
    "    pr = model_selection.cross_val_predict(pipe, features, labels, cv=5)\n",
    "    \n",
    "    print(cl)\n",
    "    print(metrics.confusion_matrix(labels, pr))\n",
    "    print(metrics.classification_report(labels, pr))\n",
    "    \n",
    "warnings.simplefilter(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier(estimators=[('tree',\n",
      "                              DecisionTreeClassifier(class_weight=None,\n",
      "                                                     criterion='entropy',\n",
      "                                                     max_depth=20,\n",
      "                                                     max_features=15,\n",
      "                                                     max_leaf_nodes=None,\n",
      "                                                     min_impurity_decrease=0.0,\n",
      "                                                     min_impurity_split=None,\n",
      "                                                     min_samples_leaf=5,\n",
      "                                                     min_samples_split=2,\n",
      "                                                     min_weight_fraction_leaf=0.0,\n",
      "                                                     presort=False,\n",
      "                                                     random_state=None,\n",
      "                                                     splitter='best')),\n",
      "                             ('rfc',\n",
      "                              RandomForestClassifier(boots...\n",
      "                                            beta_2=0.999, early_stopping=False,\n",
      "                                            epsilon=1e-08,\n",
      "                                            hidden_layer_sizes=(100,),\n",
      "                                            learning_rate='constant',\n",
      "                                            learning_rate_init=0.001,\n",
      "                                            max_iter=200, momentum=0.9,\n",
      "                                            n_iter_no_change=10,\n",
      "                                            nesterovs_momentum=True,\n",
      "                                            power_t=0.5, random_state=None,\n",
      "                                            shuffle=True, solver='adam',\n",
      "                                            tol=0.0001, validation_fraction=0.1,\n",
      "                                            verbose=False, warm_start=False))],\n",
      "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
      "                 weights=None)\n",
      "[[5101  650]\n",
      " [ 730 5021]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88      5751\n",
      "           1       0.89      0.87      0.88      5751\n",
      "\n",
      "    accuracy                           0.88     11502\n",
      "   macro avg       0.88      0.88      0.88     11502\n",
      "weighted avg       0.88      0.88      0.88     11502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voting = ensemble.VotingClassifier(classifiers)\n",
    "\n",
    "pipe = Pipeline(steps = [('smote', smote), ('preprocess', preprocessor), ('dim', decomposition.PCA()), ('classifier', voting)])\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "pr = model_selection.cross_val_predict(pipe, features, labels, cv=5)\n",
    "warnings.simplefilter(\"default\")\n",
    "    \n",
    "print(voting)\n",
    "print(metrics.confusion_matrix(labels, pr))\n",
    "print(metrics.classification_report(labels, pr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
